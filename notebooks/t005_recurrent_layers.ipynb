{
 "cells": [
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "# Tutorial X: Header\n",
    "\n",
    "In this tutorial we will cover the following\n",
    " - *What are Recurrent Neural Networks?* What is the fundamental problem they solve? LSTM, RNN building blocks, (aside on LTSM white papers, advanced methods like attention, word vecs, doc vecs, et cetera). Communication of RNN vs CNN differences\n",
    " - Flux RNN Layer Interface within Flux: [RNN](https://github.com/FluxML/Flux.jl/blob/master/src/layers/recurrent.jl#L70) [LSTM](https://github.com/FluxML/Flux.jl/blob/master/src/layers/recurrent.jl#L109) [GRU](https://github.com/FluxML/Flux.jl/blob/master/src/layers/recurrent.jl#L156)\n",
    " - Put it all together: [Char-Rnn, from model-zoo](https://github.com/FluxML/model-zoo/blob/master/text/char-rnn/char-rnn.jl)\n",
    " - Mention: Loss functions, training functions, sampling, character/word encoding\n",
    "\n",
    "Problem Statement: To Understand Flux's Recurraent NN layers\n",
    "[Backround Reading](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n",
    "[Flux Documentation](https://fluxml.ai/Flux.jl/v0.4/models/layers.html#Recurrent-Layers-1)\n",
    "# Setup\n",
    "Import the libraries here, from `Flux`, `StatsBase`, and `Base.Iterators`"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "using Flux\n",
    "using Flux: onehot, chunk, batchseq, throttle, crossentropy\n",
    "using StatsBase: wsample\n",
    "using Base.Iterators: partition\n",
    "text_file_local = \"../src/assets/t005_recurrent_layers/shakespeare.txt\"\n",
    "text_file_remote_url = \"https://cs.stanford.edu/people/karpathy/char-rnn/shakespeare_input.txt\""
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "# Download shakespeare dataset\n",
    "Run this once, per time you download the project"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "isfile(text_file_local) ||\n",
    "  download(text_file_remote_url,\n",
    "          text_file_local)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "# Set up Shakespeare dataset"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "text = collect(String(read(text_file_local)))\n",
    "alphabet = [unique(text)..., '_']\n",
    "text = map(ch -> onehot(ch, alphabet), text)\n",
    "stop = onehot('_', alphabet)\n",
    "\n",
    "N = length(alphabet)\n",
    "seqlen = 50\n",
    "nbatch = 50\n",
    "\n",
    "Xs = collect(partition(batchseq(chunk(text,        nbatch), stop), seqlen))\n",
    "Ys = collect(partition(batchseq(chunk(text[2:end], nbatch), stop), seqlen))"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "Note, the following types match, for the encoding for `Xs`, a character, and\n",
    "`Ys`, the subsequent character...\n",
    "typeof(text)                                     :: Array{Flux.OneHotVector,1}\n",
    "typeof(chunk(text,nbatch))                       :: Array{Array{Flux.OneHotVector,1},1}\n",
    "typeof(batchseq(chunk(text,nbatch), stop))       :: Array{Flux.OneHotMatrix{Array{Flux.OneHotVector,1}},1}\n",
    "typeof(collect(partition(batchseq(chunk(text,nbatch), stop), seqlen))) :: Array{Array{Flux.OneHotMatrix{Array{Flux.OneHotVector,1}},1},1}"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "# One-Hot Encoding.\n",
    "We read our shakespeare file into a text string, and apply One-Hot text encoding on it.\n",
    "Using the variables, text, and `alphabet`, the one-hot encoding scheme, reverse the\n",
    "get a String of the characters between 100 and 117"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "idx_map = collect(100:118) # collect makes an Array from a UnitRange\n",
    "string_msg = String(['a', 'b']) # Modify me !\n",
    "@assert string_msg == \"u are all resolved\""
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "# Working with OneHot Vectors\n",
    "Get the first letter of the dataset, from Xs"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "xs_letter = \"Not a letter\" # Fix Me  !!\n",
    "xs_letter = alphabet[Xs[1][1][:,1]]\n",
    "@assert alphabet[text[1]] == xs_letter"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "# Flux Recurrances\n",
    "Flux creates recurrant layers using the constructor, `Recur`\n",
    "In the following example, we will create a simple recurrant\n",
    "cell, for addition, what will be the result of applying it to\n",
    "the sequence of numbers from 1:10 ?"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "cell(h, x) = (h + x, x)\n",
    "rnn = Flux.Recur(cell, 0)\n",
    "rnn(collect(1:100))\n",
    "\n",
    "rnn_state = 0 # Modify me !\n",
    "@assert rnn.state = rnn_state"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "# Flux Recurance, many dimensions\n",
    "Given a `n` dimensional input, create a `Flux.Recur` cal\n",
    "modifying the code above"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "n = 10\n",
    "seq_len = 20\n",
    "input_data = ones(n, seq_len)\n",
    "\n",
    "cell(h, x) = (h .+ x, x)\n",
    "rnn = Flux.Recur(cell, zeros(n))\n",
    "\n",
    "rnn.(input_data) # modify the shape of the input data\n",
    "@assert ones(seq_len)*10 == rnn.state"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "Note, the desired input type passed to rnn is going to be `Array{Array{Float64,1},1}``"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "# RNN - Basic layers\n",
    "apply the layer to Xs[1]"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "layer = RNN(N, 1)\n",
    "layer.(Xs[1])"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "# Adding Layers to Models\n",
    "Given a model, `m`, a loss function"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "m = Chain(\n",
    "  LSTM(N, 128),\n",
    "  LSTM(128, 128),\n",
    "  Dense(128, N),\n",
    "  softmax)\n",
    "\n",
    "\n",
    "function loss(xs, ys)\n",
    "  l = sum(crossentropy.(m.(xs), ys))\n",
    "  Flux.truncate!(m)\n",
    "  return l\n",
    "end\n",
    "\n",
    "opt = ADAM(0.01)\n",
    "tx, ty = (Xs[5], Ys[5])\n",
    "evalcb = () -> @show loss(tx, ty)\n",
    "\n",
    "params_m = params(m)\n",
    "\n",
    "Flux.train!(loss = NaN, params = NaN, data = NaN, opts = NaN, cb = throttle(evalcb, 30)) # Fill in the first 4 args\n",
    "\n",
    "@assert params_m != params(m)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "*This notebook was generated using [Literate.jl](https://github.com/fredrikekre/Literate.jl).*"
   ],
   "metadata": {}
  }
 ],
 "nbformat_minor": 3,
 "metadata": {
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.2.0"
  },
  "kernelspec": {
   "name": "julia-1.2",
   "display_name": "Julia 1.2.0",
   "language": "julia"
  }
 },
 "nbformat": 4
}
